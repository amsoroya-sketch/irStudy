# AI-Powered Medical Education System
## ICRP + AMC MCQ + AMC Clinical Preparation

A comprehensive, AI-powered platform for medical exam preparation using local LLMs, RAG, and multi-agent systems.

---

## ğŸ¯ Project Overview

This system provides:
- **ICRP (Young Hospital, NSW)** preparation materials
- **AMC MCQ (CAT)** comprehensive question bank and study materials
- **AMC Clinical (OSCE)** scenario simulation and practice

### Key Features:
- âœ… **100% Self-Hosted** - No cloud costs, no API fees
- âœ… **Local LLMs** - Uses Ollama with medical-specific models
- âœ… **RAG System** - Semantic search across 20+ medical textbooks
- âœ… **Multi-Agent AI** - Specialized agents for each medical specialty
- âœ… **Unlimited Content** - AI generates questions on-demand
- âœ… **Australian-Focused** - Therapeutic Guidelines, NSW protocols, AMC format

---

## ğŸ“‹ Prerequisites

### System Requirements:
- **OS:** Linux (Ubuntu 20.04+ recommended)
- **GPU:** NVIDIA GPU with CUDA support (for local LLM inference)
- **RAM:** 32GB+ recommended (16GB minimum)
- **Storage:** 100GB+ free space
- **Python:** 3.11+
- **Docker:** Latest version

### Software:
- Ollama (for local LLMs)
- Docker & Docker Compose
- NVIDIA CUDA drivers
- Tesseract OCR (for scanned PDFs)

---

## ğŸš€ Quick Start

### 1. Clone Repository
```bash
git clone <your-repo-url>
cd irStudy
```

### 2. Install Python Dependencies
```bash
# Create virtual environment
python3 -m venv venv
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt

# Install Tesseract OCR
sudo apt-get install tesseract-ocr
```

### 3. Download Additional Ollama Models
```bash
# Medical-specific models
ollama pull meditron:7b          # Medical expert
ollama pull biomistral:7b        # Biomedical LLM
ollama pull llama3.1:70b         # Best reasoning
ollama pull mixtral:8x7b         # Question generation

# Verify installation
ollama list
```

### 4. Start Infrastructure Services
```bash
# Start Qdrant, Neo4j, PostgreSQL, Redis, Prometheus, Grafana
docker-compose up -d

# Verify all services are running
docker-compose ps
```

### 5. Acquire Medical Textbooks (PDF)
Place your medical textbook PDFs in the following structure:
```
data/pdfs/
â”œâ”€â”€ australian/
â”‚   â”œâ”€â”€ therapeutic_guidelines_etg.pdf
â”‚   â”œâ”€â”€ murtaghs_general_practice_8ed.pdf
â”‚   â”œâ”€â”€ talley_clinical_examination_9ed.pdf
â”‚   â””â”€â”€ amh_australian_medicines_handbook.pdf
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ harrisons_internal_medicine_21ed.pdf
â”‚   â”œâ”€â”€ kumar_clark_clinical_medicine_10ed.pdf
â”‚   â””â”€â”€ davidsons_medicine_24ed.pdf
â”œâ”€â”€ specialties/
â”‚   â”œâ”€â”€ nelson_pediatrics_21ed.pdf
â”‚   â”œâ”€â”€ williams_obstetrics_26ed.pdf
â”‚   â”œâ”€â”€ bailey_love_surgery_28ed.pdf
â”‚   â””â”€â”€ kaplan_sadock_psychiatry_12ed.pdf
â””â”€â”€ free/
    â””â”€â”€ statpearls.pdf
```

---

## ğŸ“š Processing Medical Textbooks

### Step 1: Extract Text from PDFs
```bash
# Extract text from all PDFs (handles OCR for scanned pages)
python scripts/extract_pdfs.py \
    --input data/pdfs \
    --output data/processed

# This will create JSON files in data/processed/
# Expected time: 1-3 hours for 20 books
```

### Step 2: Chunk Medical Texts
```bash
# Intelligently chunk texts while preserving medical context
python scripts/chunk_medical_texts.py \
    --input data/processed \
    --output data/chunks.json \
    --chunk-size 1000 \
    --overlap 150

# Expected output: 30,000-50,000 chunks
# Expected time: 10-30 minutes
```

### Step 3: Generate Embeddings
```bash
# Generate PubMedBERT embeddings (using local GPU)
python scripts/generate_embeddings.py \
    --input data/chunks.json \
    --output data/embeddings/medical_embeddings.pkl \
    --model pritamdeka/S-PubMedBert-MS-MARCO \
    --batch-size 32

# Expected time: 2-4 hours for 40,000 chunks on GPU
# File size: ~500MB
```

### Step 4: Index in Qdrant
```bash
# Upload embeddings to Qdrant vector database
python scripts/index_qdrant.py \
    --embeddings data/embeddings/medical_embeddings.pkl \
    --collection medical_knowledge \
    --qdrant-url http://localhost:6333 \
    --batch-size 100

# Verify indexing with test search
python scripts/index_qdrant.py --test

# Expected time: 20-40 minutes
```

---

## ğŸ§ª Testing the System

### Test Local LLM Integration
```bash
python src/models/ollama_client.py
```

### Test RAG Search
```python
from qdrant_client import QdrantClient
from sentence_transformers import SentenceTransformer

# Connect to Qdrant
client = QdrantClient(url="http://localhost:6333")

# Load embedding model
model = SentenceTransformer('pritamdeka/S-PubMedBert-MS-MARCO')

# Search medical knowledge
query = "Management of acute coronary syndrome"
query_embedding = model.encode(query).tolist()

results = client.search(
    collection_name="medical_knowledge",
    query_vector=query_embedding,
    limit=5
)

for result in results:
    print(f"Score: {result.score:.4f}")
    print(f"Source: {result.payload['source']}")
    print(f"Text: {result.payload['text'][:200]}...\n")
```

---

## ğŸ“‚ Project Structure

```
irStudy/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ pdfs/                    # Medical textbook PDFs (you provide)
â”‚   â”‚   â”œâ”€â”€ australian/
â”‚   â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ specialties/
â”‚   â”‚   â””â”€â”€ free/
â”‚   â”œâ”€â”€ processed/               # Extracted text (JSON)
â”‚   â”œâ”€â”€ embeddings/              # Generated embeddings
â”‚   â””â”€â”€ chunks.json              # All text chunks
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ extract_pdfs.py          # PDF text extraction
â”‚   â”œâ”€â”€ chunk_medical_texts.py   # Text chunking
â”‚   â”œâ”€â”€ generate_embeddings.py   # Embedding generation
â”‚   â””â”€â”€ index_qdrant.py          # Qdrant indexing
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agents/                  # Multi-agent system (TBD)
â”‚   â”œâ”€â”€ rag/                     # RAG system (TBD)
â”‚   â”œâ”€â”€ api/                     # FastAPI backend (TBD)
â”‚   â””â”€â”€ models/
â”‚       â””â”€â”€ ollama_client.py     # Local LLM integration
â”œâ”€â”€ docker/                      # Docker volumes
â”œâ”€â”€ docker-compose.yml           # Infrastructure services
â”œâ”€â”€ requirements.txt             # Python dependencies
â””â”€â”€ README.md                    # This file
```

---

## ğŸ”§ Infrastructure Services

### Qdrant (Vector Database)
- **URL:** http://localhost:6333
- **Dashboard:** http://localhost:6333/dashboard
- **Purpose:** Semantic search across medical knowledge

### Neo4j (Knowledge Graph)
- **URL:** http://localhost:7474
- **Username:** neo4j
- **Password:** medical_ai_password_2025
- **Purpose:** Medical entity relationships (diseases, symptoms, treatments)

### PostgreSQL (Relational Database)
- **Host:** localhost:5432
- **Database:** medical_education
- **Username:** medical_user
- **Password:** medical_pass_2025
- **Purpose:** User data, questions, progress tracking

### Redis (Cache & Queue)
- **URL:** localhost:6379
- **Purpose:** Caching, background jobs (Celery)

### Prometheus (Metrics)
- **URL:** http://localhost:9090
- **Purpose:** System metrics collection

### Grafana (Dashboards)
- **URL:** http://localhost:3001
- **Username:** admin
- **Password:** medical_admin_2025
- **Purpose:** Monitoring dashboards

---

## ğŸ’° Cost Analysis

### Development Costs: **$0**
- âœ… Local LLMs (Ollama)
- âœ… Self-hosted infrastructure
- âœ… Open-source software

### Operational Costs: **$0/month**
- âœ… No cloud fees
- âœ… No API costs
- âœ… Electricity only (~$50-100/month)

### Only Real Cost:
- Medical textbooks: **$800-2,000** (one-time)
- OR use free resources: **$0**

---

## ğŸ“– Essential Medical Textbooks

### Must-Have (Top 5):
1. **Therapeutic Guidelines (eTG)** - $385/year
2. **Talley & O'Connor Clinical Examination** - $130
3. **AMC Handbook of MCQs** - $180
4. **Murtagh's General Practice** - $180
5. **Australian Medicines Handbook** - $155

### Comprehensive (20 books):
- See full list in implementation plan
- **Total:** ~$2,000-3,000 (one-time purchase)

### Free Alternatives:
- StatPearls (free comprehensive medical reference)
- NCBI Bookshelf (free medical books)
- PubMed Central (3+ million free articles)
- Australian government guidelines (all free)

---

## ğŸ¯ Next Steps

### Immediate (This Week):
1. âœ… Install all dependencies
2. âœ… Start Docker services
3. âœ… Download Ollama models
4. â³ Acquire medical textbook PDFs
5. â³ Extract and process PDFs
6. â³ Generate embeddings and index in Qdrant

### Short-term (Next 2 Weeks):
- Build RAG query system
- Create first medical expert agent
- Generate test MCQ questions
- Validate with medical professional

### Medium-term (1-2 Months):
- Complete multi-agent system (28 agents)
- Generate 5,000+ MCQs
- Build FastAPI backend
- Create Next.js frontend

### Long-term (3-6 Months):
- 18,000+ MCQs across all exams
- 3,000+ clinical scenarios
- Full platform with adaptive learning
- Beta testing and launch

---

## ğŸ¤ Contributing

This is a personal medical education project. If you're a medical professional interested in validating content or contributing, please reach out!

---

## ğŸ“„ License

Private project. All rights reserved.

---

## ğŸ†˜ Troubleshooting

### Ollama not responding
```bash
# Check Ollama status
systemctl status ollama

# Restart Ollama
systemctl restart ollama
```

### GPU not detected
```bash
# Check CUDA
nvidia-smi

# Verify PyTorch CUDA
python -c "import torch; print(torch.cuda.is_available())"
```

### Docker services not starting
```bash
# Check logs
docker-compose logs <service-name>

# Restart all services
docker-compose down
docker-compose up -d
```

### Out of memory during embedding generation
```bash
# Reduce batch size
python scripts/generate_embeddings.py --batch-size 16
```

---

## ğŸ“ Support

For issues or questions about this project, please create an issue in the repository.

---

**Last Updated:** December 14, 2025
**Version:** 0.1.0-alpha
**Status:** Active Development
